% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MulGroupsCros.R
\name{MulGroupsCros}
\alias{MulGroupsCros}
\title{Build multigroups model}
\usage{
eSet = MulGroupsCros(eSet = eSet,Groups = c("Exposure","Biomarker"),
                    VarsY = c("Y1"),VarsC = "all.c",TuneMethod = "grid_search",TuneNum = 5,RsmpMethod = "cv",
                    Folds = 5,VarsImpThr = 0.85,SG_Lrns = c("rf","xgboost"))
}
\arguments{
\item{eSet}{An R6 class object}

\item{Groups}{chr. Groups to be integrated. The groups of outcome and covariates or
confounders are not included. Note that separates different learners by "," and without space(e.g. Groups = "exposure, biomarker").}

\item{VarsY}{chr. Outcome variable for modelling. Only one variable can be entered.}

\item{VarsC}{chr. Covariates needing further statistical test. "all.c" option refers to all covariate variables listed in the data file.
Users can also select part of them by copying available vars. Note that separates different vars by "," and without space(e.g. VarsC = "C1,C2").}

\item{TuneMethod}{chr. Method for hyper-parameter autotuning.
Options include "default", "random_search", "grid_search", "nloptr"(Non-linear optimization), and "gensa"(Generalized simulated annealing).
The "default" option uses the simple training method for parameter optimization of mlr3 package.}

\item{TuneNum}{num. Upper limit of model tuning times. It should be more than 20 times to search the appropriate parameters,
but it takes more time. In theory, more time, better training results.}

\item{RsmpMethod}{chr. Method for resampling. Options include "cv"(cross validation), "loo"(leave-one-out cross validation),
"bootstrap"(bootstrapping), "holdout"(holdout).}

\item{Folds}{num. Folds for cross validation resampling method. The default value is 5.}

\item{Ratio}{num. Ratio for "Holdout" resampling method. The default value is 5.}

\item{Repeats}{num. Repeats for "Bootstrap" resampling method.}

\item{VarsImpThr}{num. Threshold for feature selection.
It refers to the ratio of accumulated importance of all variables of the selected variables for building the final model.}

\item{SG_Lrns}{chr. Learners for stacked generalization. Options include "lasso", "enet"(Elastic net), "rf"(Random forest), and "xgboost"(Xgboost).
One or more arbitrary options can be selected at the same time. Note that separates different learners by "," and without space(e.g. SG_Lrns ="lasso,enet,rf,xgboost").}
}
\value{
An R6 class object containing eight elements. The elements of that object include:
(1) "Importance": A list containing dataframes that contain the importance of features after modeling a single group from different groups.
(2) "Feature": A list containing dataframes that contain the the coefficients or importance of selected features after modeling a single group from different learners.
(3) "Feature_select": A list containing dataframes that contain the selected features after modeling a single group from different learners.
(4) "ModelStat": A list containing dataframes that contain the r-square value of the single group model built by different learners.
(5) "Prediction_comp": A list containing dataframes that contain the prediction values of the SG model built by different combinations of learners.
(6) "SGModel_summary": A dataframe containing the r-square value of the SG model built by different combinations of learners.
(7) "NodeNum": The node number generated by different models.
(8) "SGplot": A visualized plot for SG model summary.
}
\description{
MulGroupsCros function is designed to integrate the multi-group data to predict the incidence risk.
It mainly aims to construct various stacked generalization models to predict the probability of outcome
incidence, as well as providing the statistical explanation.
}
\details{
The calculation time depends on the characteristics of your data, the number of learning methods,
and the tuning method. For parameter "TuneMethod", the default option can provide faster calculations but less accurate
results than other autotune methods. If you want to train a better model, choose other auto-tune method and increase the number of tuning times.
}
\examples{
   eSet = InitEX(PID = "EX", FileDirIn = "default", FileDirOut = "default")
eSet = LoadEX(eSet = eSet,UseExample = "default",FileDirExpo = "examdata.xlsx",FileDirVoca = "examvoca.xlsx")
eSet = TransImput(eSet = eSet,Group = T,Vars = "all.e",Method = "lod")
eSet = DelLowVar(eSet = eSet)
eSet = DelMiss(eSet = eSet)
eSet = TransType(eSet = eSet,Vars = c("Y1", "C1","E203","E204","E207","E209"),TypeTo = "factor")
eSet = TransScale(eSet = eSet,Group = T,Vars = "all.e",Method = "normal",
                  Direct = "positive",RangeLow = 0,RangeUpper = 1)
eSet = TransDistr(eSet = eSet,Vars = c("C2", "C3"), Method = "log2")
eSet = TransDummy(eSet = eSet,Vars = "default")
eSet = MulGroupsCros(eSet = eSet,Groups = c("Exposure","Biomarker"),
                    VarsY = c("Y1"),VarsC = "all.c",TuneMethod = "grid_search",TuneNum = 5,RsmpMethod = "cv",
                    Folds = 5,VarsImpThr = 0.85,SG_Lrns = c("rf","xgboost"))
}
\author{
Guohuan Zhang, Yuting Wang, Bin Wang (corresponding author)
}
